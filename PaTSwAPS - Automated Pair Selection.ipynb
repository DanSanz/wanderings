{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PaTSwAPS: Pairs Trading Strategy with Automated Pair Selection\n",
    "#*Pair Selector Notebook*\n",
    "##To-Do\n",
    "\n",
    "1) Read this paper. The authors manage to find cointegrated pairs and simulate a successful trading strategy. They give decent advice on how they did their screening. http://www.ccsenet.org/journal/index.php/ijef/article/view/33007\n",
    "\n",
    "2) Test for cointegration over many time periods (i.e. days, weeks, months, years)\n",
    "\n",
    "3) Research how to correct for the multiple comparisons problem. https://en.wikipedia.org/wiki/Multiple_comparisons_problem#Controlling_procedures\n",
    "\n",
    "4) Consider using the following more advanced statistical tests for cointegration:\n",
    "* Augmented-Dickey Fuller test \n",
    "* Hurst exponent\n",
    "* Half-life of mean reversion inferred from an Ornstein–Uhlenbeck process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correcting for Multiple Comparisons\n",
    "\n",
    "For references and more information, see https://en.wikipedia.org/wiki/Multiple_comparisons_problem#Controlling_procedures\n",
    "\n",
    "Let $\\bar{\\alpha}$ denote the *family-wise error rate* (FWER), or the experiment-wide significance level, which is the probability of making one or more false positives (type I errors), or identifying a cointegrated pair when there is no underlying cointegration present.\n",
    "\n",
    "Furthermore, let $\\alpha_{comp}$ denote the significance level of each individual trial (or comparison).\n",
    "\n",
    "Then, the following relationship holds: $\\bar{\\alpha}=1-(1-\\alpha_{comp})^k$\n",
    "\n",
    "Below are several ways of correcting for multiple comparisons:\n",
    "\n",
    "* Bonferroni correction: Let $\\alpha_{comp}=\\bar{\\alpha}/k$. This correction is just an approximate solution for the per-comparison significance level, using the binomial theorem. The approximation gets better with small $\\alpha_{comp}$ and $k$. For more information, see https://en.wikipedia.org/wiki/Bonferroni_correction#Definition\n",
    "\n",
    "\n",
    "* Šidák correction: Let $\\alpha_{comp}=1-(1-\\bar{\\alpha})^{1/k}$. This correction is an exact solution for the per-comparison significance level. For more information, see https://en.wikipedia.org/wiki/%C5%A0id%C3%A1k_correction#Usage\n",
    "\n",
    "\n",
    "* Holm-Bonferroni correction: Let $\\alpha_{comp}=\\bar{\\alpha}/(k-i+1)$. This correction is a more powerful method than the simple Bonferroni correction, but is more complex in that it is a stepwise algorithm. In essence, it tests the most extreme p-value against the most stringent criteria $(i=1)$, and tests progressively less extreme p-values against progressively less strict criteria $(i>1)$. For more information, see https://en.wikipedia.org/wiki/Holm%E2%80%93Bonferroni_method#Formulation\n",
    "\n",
    "We implement the Šidák correction, as it is the most robust correction that can be implemented in a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from datetime import date, timedelta\n",
    "from statsmodels.tsa.stattools import coint\n",
    "from quantopian.pipeline import CustomFactor, Pipeline\n",
    "from quantopian.pipeline.factors import SimpleMovingAverage, AverageDollarVolume\n",
    "from quantopian.pipeline.classifiers.morningstar import Sector\n",
    "from quantopian.pipeline.data import morningstar\n",
    "from quantopian.pipeline.data.builtin import USEquityPricing\n",
    "from quantopian.research import run_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function takes a dataframe of prices (price vs. time) and returns a list of any pairs that are cointegrated.\n",
    "\n",
    "def find_cointegrated_pairs(data):    \n",
    "    # Drop duplicated rows and set up necessary variables\n",
    "    data = data.T.drop_duplicates().T\n",
    "    m, n = data.shape[0], data.shape[1]\n",
    "    pvalue_matrix = np.zeros((n, n))\n",
    "    keys = data.keys()\n",
    "    pairs = []\n",
    "    \n",
    "    # Make a matrix of p-values\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            result = coint(data[keys[i]], data[keys[j]])\n",
    "            pvalue_matrix[i, j] = result[1]\n",
    "    \n",
    "    # Find uniquely cointegrated pairs of securities\n",
    "    alpha = sidak(0.05, n*(n-1)/2)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            check1 = (pvalue_matrix[k, j] >= 0.5 for k in range(i))\n",
    "            check2 = (pvalue_matrix[i, k] >= 0.5 for k in range(j, n))\n",
    "            #check3 = (not math.isnan(float(pvalue_matrix[k, j])) for k in range(i))\n",
    "            #check4 = (not math.isnan(float(pvalue_matrix[i, k])) for k in range(j, n))\n",
    "            if (pvalue_matrix[i, j] <= alpha) and check1 and check2: #and check3 and check4:\n",
    "                pairs.append((keys[i].symbol, keys[j].symbol))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "def sidak(fwer, num_comps):\n",
    "    return np.float128(1-(1-fwer)**(1.0/num_comps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'ABGB', u'FSLR')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Interesting cointegrated pairs to keep track of!\n",
    "foo = get_pricing(['CSUN', 'ASTI', 'ABGB', 'FSLR'], '01-01-2014', '01-01-2015', fields='price')\n",
    "find_cointegrated_pairs(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define and instantiate all necessary factors\n",
    "\n",
    "class Market_Cap(CustomFactor):\n",
    "    inputs = [morningstar.valuation.market_cap]\n",
    "    window_length = 1\n",
    "    def compute(self, today, assets, out, inputs):\n",
    "        out[:] = inputs\n",
    "\n",
    "class Industry_Group(CustomFactor):\n",
    "    inputs = [morningstar.asset_classification.morningstar_industry_group_code]\n",
    "    window_length = 1\n",
    "    def compute(self, today, assets, out, inputs):\n",
    "        out[:] = inputs\n",
    "\n",
    "avg_close = SimpleMovingAverage(inputs=[USEquityPricing.close], window_length=20)\n",
    "avg_vol = AverageDollarVolume(window_length=20)\n",
    "sector = Sector()\n",
    "group = Industry_Group()\n",
    "market_cap = Market_Cap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The Pipeline filters the universe by group code, and applies minimum acceptance requirements (enumerated below)\n",
    "\n",
    "def make_pipeline(group_code):\n",
    "    sector_filter = sector.notnull() # No stocks in misc. sector\n",
    "    penny_stock_filter = (avg_close > 5.0) # No stocks that are less that $5\n",
    "    volume_filter = (avg_vol > 750000) # No companies who have a dollar volume of less than $0.75m\n",
    "    small_cap_filter = (market_cap >= 300000000) # No companies who are valued at less than $300m\n",
    "    group_filter = group.eq(group_code) # No companies that are not in the industry group under consideration\n",
    "    \n",
    "    return Pipeline(\n",
    "        columns = {'industry_group':group},\n",
    "        screen = (sector_filter & penny_stock_filter & volume_filter & small_cap_filter & group_filter)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Morningstar industry group codes: for mappings, see https://www.quantopian.com/help/fundamentals#industry-sector\n",
    "\n",
    "group_codes = [10101, 10102, 10103, 10104, 10105, 10106, 10107,\n",
    "               10208, 10209, 10210, 10211, 10212, 10213, 10214, 10215, 10216, 10217, 10218,\n",
    "               10319, 10320, 10321, 10322, 10323, 10324, 10325, 10326,\n",
    "               10427, 10428,\n",
    "               20529, 20530, 20531, 20532, 20533, 20534,\n",
    "               20635, 20636, 20637, 20638, 20639, 20640, 20641, 20642,\n",
    "               20743, 20744,\n",
    "               30845,\n",
    "               30946, 30947, 30948, 30949, 30950, 30951,\n",
    "               31052, 31053, 31054, 31055, 31056, 31057, 31058, 31059, 31060, 31061, 31062, 31063, 31064,\n",
    "               31165, 31166, 31167, 31168, 31169]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'VIAB', u'VIA'),\n",
       " (u'RYL', u'BZH'),\n",
       " (u'LEG', u'SCSS'),\n",
       " (u'WTW', u'STON'),\n",
       " (u'RENX', u'RELX'),\n",
       " (u'VKI', u'MMD'),\n",
       " (u'NXZ', u'MAIN'),\n",
       " (u'NXZ', u'MMD'),\n",
       " (u'EIM', u'APO'),\n",
       " (u'TMP', u'UBNK'),\n",
       " (u'UBNK', u'WAL'),\n",
       " (u'UBNK', u'HOMB'),\n",
       " (u'UBNK', u'TREE'),\n",
       " (u'UBNK', u'BSBR'),\n",
       " (u'UBNK', u'STBZ'),\n",
       " (u'UBNK', u'FRC'),\n",
       " (u'UBNK', u'WD'),\n",
       " (u'UBNK', u'BSMX'),\n",
       " (u'ANAT', u'BRK_A'),\n",
       " (u'ANAT', u'HMN'),\n",
       " (u'ANAT', u'ORI'),\n",
       " (u'ANAT', u'KMPR'),\n",
       " (u'ANAT', u'BRK_B'),\n",
       " (u'ANAT', u'HIG'),\n",
       " (u'ANAT', u'ING'),\n",
       " (u'ANAT', u'SLF'),\n",
       " (u'ANAT', u'AIZ'),\n",
       " (u'ANAT', u'GNW'),\n",
       " (u'ANAT', u'ESGR'),\n",
       " (u'ANAT', u'GTS'),\n",
       " (u'ANAT', u'AV'),\n",
       " (u'BRK_A', u'BRK_B'),\n",
       " (u'GLPI', u'CTT'),\n",
       " (u'BF_A', u'BF_B'),\n",
       " (u'NBIX', u'SCMP'),\n",
       " (u'MCK', u'ABC'),\n",
       " (u'LVLT', u'SJR'),\n",
       " (u'ZNH', u'UAL'),\n",
       " (u'TAL', u'HEES'),\n",
       " (u'TAL', u'AYR'),\n",
       " (u'TAL', u'HRI'),\n",
       " (u'TAL', u'CAI'),\n",
       " (u'TAL', u'FLY'),\n",
       " (u'TAL', u'TGH'),\n",
       " (u'TAL', u'MG'),\n",
       " (u'TAL', u'AL'),\n",
       " (u'TAL', u'ADT'),\n",
       " (u'TAL', u'ALLE'),\n",
       " (u'AZZ', u'ZBRA'),\n",
       " (u'AZZ', u'HON'),\n",
       " (u'CPLP', u'KNOP'),\n",
       " (u'OSK', u'HY')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This code goes through accepted stocks in each industry group and find if any stocks are cointegrated over the\n",
    "# past 365 days\n",
    "\n",
    "pairs = []\n",
    "start = '01-01-2014'\n",
    "end = '01-01-2015'\n",
    "\n",
    "for i in range(len(group_codes)):\n",
    "    symbols = []\n",
    "    pipe_output = run_pipeline(make_pipeline(group_codes[i]), end, end)\n",
    "    for j in range(len(pipe_output.index)):\n",
    "        symbols.append(pipe_output.index.values[j][1].symbol)\n",
    "    if symbols != []:\n",
    "        prices = get_pricing(symbols, start, end, fields='price')\n",
    "        prices.dropna(axis=1)\n",
    "        pairs = pairs + find_cointegrated_pairs(prices)\n",
    "\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'TMP', u'UBNK'),\n",
       " (u'UBNK', u'WAL'),\n",
       " (u'UBNK', u'HOMB'),\n",
       " (u'UBNK', u'TREE'),\n",
       " (u'UBNK', u'BSBR'),\n",
       " (u'UBNK', u'STBZ'),\n",
       " (u'UBNK', u'FRC'),\n",
       " (u'UBNK', u'WD'),\n",
       " (u'UBNK', u'BSMX')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs = []\n",
    "start = '01-01-2014'\n",
    "end = '01-01-2015'\n",
    "\n",
    "i = 19\n",
    "symbols = []\n",
    "pipe_output = run_pipeline(make_pipeline(group_codes[i]), end, end)\n",
    "for j in range(len(pipe_output.index)):\n",
    "    symbols.append(pipe_output.index.values[j][1].symbol)\n",
    "if symbols != []:\n",
    "    prices = get_pricing(symbols, start, end, fields='price')\n",
    "    prices.dropna(axis=1)\n",
    "    pairs = pairs + find_cointegrated_pairs(prices)\n",
    "\n",
    "pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}